{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORFLOW CHECK\n",
    "\"\"\"\n",
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE FILES IF THEY HAVEN'T\n",
    "!python3 minesweeper.py\n",
    "!python3 DQNsetup.py\n",
    "# from minesweeper_env import *\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import minesweeper\n",
    "from minesweeper import Board\n",
    "import DQNsetup\n",
    "from DQNsetup import *\n",
    "import model_tensorboard\n",
    "from model_tensorboard import *\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c532a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "MEM_SIZE = 50_000 # number of moves to store in replay buffer\n",
    "MEM_SIZE_MIN = 1_000 # min number of moves in replay buffer\n",
    "episodes = 100_000\n",
    "\n",
    "# Learning settings\n",
    "BATCH_SIZE = 64\n",
    "learn_rate = 0.01\n",
    "LEARN_DECAY = 0.99975\n",
    "LEARN_MIN = 0.001\n",
    "DISCOUNT = 0.1 #gamma\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 0.95\n",
    "EPSILON_DECAY = 0.99975\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "# DQN settings\n",
    "CONV_UNITS = 64 # number of neurons in each conv layer\n",
    "DENSE_UNITS = 512 # number of neurons in fully connected dense layer\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "\n",
    "# Default model name\n",
    "MODEL_NAME = f'conv{CONV_UNITS}x4_dense{DENSE_UNITS}x2_y{DISCOUNT}_minlr{LEARN_MIN}'\n",
    "\n",
    "AGG_STATS_EVERY = 100 # calculate stats every 100 games for tensorboard\n",
    "SAVE_MODEL_EVERY = 10_000 # save model and replay every 10,000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING EVERTHING CELL\n",
    "\n",
    "# Initialize the Board (4x4 matrix)\n",
    "env = Board(9, 9)\n",
    "\n",
    "# Set the mines about (2,2)\n",
    "# Assume user clicked coordinate (2,2) as the first tile\n",
    "f_row, f_col = np.random.randint(env.rows, size=2)\n",
    "print(\"First row: %d, First Col: %d\" % (f_row, f_col))\n",
    "env.set_mines_about(f_row, f_col,10) # set_mines_about(self,row_center,col_center,num_mines)\n",
    "print(\"Mines: \")\n",
    "env.printMines()\n",
    "print(\"Board: \")\n",
    "env.printBoard()\n",
    "state_im = env.board3D() # board is currently 2D, making it 3D by (row, col, 1)\n",
    "\n",
    "# Initialize the model \n",
    "model = DQN_setup(learn_rate, state_im.shape, env.ntiles, CONV_UNITS, DENSE_UNITS)\n",
    "\n",
    "# Initialize the model that would always be ahead (looking at the future)\n",
    "target_model = DQN_setup(learn_rate, state_im.shape, env.ntiles, CONV_UNITS, DENSE_UNITS)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=MEM_SIZE)\n",
    "target_update_counter = 0\n",
    "\n",
    "tensorboard = ModifiedTensorBoard(\n",
    "            log_dir=f'logs\\\\{MODEL_NAME}', profile_batch=0)\n",
    "\n",
    "progress_list, wins_list, ep_rewards = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2316d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAY THE GAME!!! (# episodes Games)\n",
    "for episode in tqdm(range(1,episodes+1), unit='episode'):\n",
    "    tensorboard.step = episode\n",
    "    \n",
    "    env.reset()\n",
    "    f_row, f_col = np.random.randint(env.rows, size=2)\n",
    "    env.set_mines_about(f_row, f_col,15)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    \n",
    "    past_n_wins = env.n_wins\n",
    "\n",
    "    # play until lose\n",
    "    while not done:\n",
    "        \n",
    "        current_state = env.board3D()\n",
    "        \n",
    "        # get action\n",
    "        board = env.board3D().reshape(1, env.ntiles)\n",
    "        # print(\"Board: \", board)\n",
    "\n",
    "        # Select the unopened tiles\n",
    "        unopened_tiles = [i for i, value in enumerate(board[0]) if value==-1]\n",
    "        # print(\"Unopened Tiles: \", unopened_tiles)\n",
    "\n",
    "        rand = np.random.random() # number from 0 to 1\n",
    "\n",
    "        if rand < epsilon:\n",
    "            # print(\"\\nUsed Random\")\n",
    "            move = np.random.choice(unopened_tiles)\n",
    "        else:\n",
    "            # print(\"\\nUsed Model To Predict\")\n",
    "            moves = model.predict(np.reshape(current_state, (1, env.rows, env.cols, 1)))\n",
    "            # print(type(moves))\n",
    "            # print(\"moves:\", moves)\n",
    "            # Disregard all the opened tiles\n",
    "            moves[board!=-1] = np.min(moves)\n",
    "            # Pick a tile with the best probability\n",
    "            move = np.argmax(moves)\n",
    "\n",
    "\n",
    "        # print(\"\\naction: \", move)\n",
    "        # print(\"Board: \", env.board)\n",
    "        # print(\"Mines: \", env.mines)\n",
    "\n",
    "        # Retrieve the next step and reward\n",
    "        new_state, reward, done = env.dig(math.floor(move / env.cols), move % env.cols)\n",
    "        # print(\"\\nREWARD: \", reward)\n",
    "        ep_reward += reward\n",
    "\n",
    "        # append the data to batch_array\n",
    "        replay_memory.append((current_state, move, reward, new_state, done))\n",
    "\n",
    "        # Train\n",
    "        if len(replay_memory) < MEM_SIZE_MIN:\n",
    "            # print(\"SKIP in Training Process\")\n",
    "            continue\n",
    "\n",
    "        # Get the random sample of batches\n",
    "        batch = random.sample(replay_memory, BATCH_SIZE)\n",
    "\n",
    "        # Q table for current model using the batches\n",
    "        current_states = np.array([transition[0] for transition in batch])\n",
    "        current_qs_list = model.predict(current_states)\n",
    "        # print(current_qs_list.shape)\n",
    "        # print(\"\\nCurrent Q Table: \", current_qs_list)\n",
    "\n",
    "        # Q table for future model\n",
    "        new_current_states = np.array([transition[3] for transition in batch])\n",
    "        future_qs_list = target_model.predict(new_current_states)\n",
    "        # print(future_qs_list.shape)\n",
    "        # print(\"\\nFuture Q Table: \", future_qs_list)\n",
    "        \n",
    "        X_train = [] # Feature\n",
    "        Y_train = [] # Label\n",
    "\n",
    "        for i, (current_state, action, reward, new_current_states, done) in enumerate(batch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[i])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[i]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X_train.append(current_state)\n",
    "            Y_train.append(current_qs)\n",
    "\n",
    "        model.fit(np.array(X_train), np.array(Y_train), batch_size=BATCH_SIZE,\n",
    "                  shuffle=False, verbose=0, callbacks=[tensorboard]\\\n",
    "                  if done else None)\n",
    "\n",
    "        # updating to determine if we want to update target_model yet\n",
    "        if done:\n",
    "            target_update_counter += 1\n",
    "\n",
    "        if target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "            target_update_counter = 0\n",
    "\n",
    "        # decay learn_rate\n",
    "        learn_rate = max(LEARN_MIN, learn_rate*LEARN_DECAY)\n",
    "\n",
    "        # decay epsilon\n",
    "        epsilon = max(EPSILON_MIN, epsilon*EPSILON_DECAY)\n",
    "        \n",
    "    progress_list.append(env.n_progress) # n of non-guess moves\n",
    "    ep_rewards.append(ep_reward)\n",
    "    \n",
    "    # print(\"Number of Wins :\", env.n_wins)\n",
    "    if env.n_wins > past_n_wins:\n",
    "        wins_list.append(1)\n",
    "    else:\n",
    "        wins_list.append(0)\n",
    "\n",
    "    if len(replay_memory) < MEM_SIZE_MIN:\n",
    "        # print(\"SKIP after Training Process\")\n",
    "        continue\n",
    "\n",
    "    if not episode % AGG_STATS_EVERY:\n",
    "        med_progress = round(np.median(progress_list[-AGG_STATS_EVERY:]), 2)\n",
    "        win_rate = round(np.sum(wins_list[-AGG_STATS_EVERY:]) / AGG_STATS_EVERY, 2)\n",
    "        med_reward = round(np.median(ep_rewards[-AGG_STATS_EVERY:]), 2)\n",
    "\n",
    "        tensorboard.update_stats(\n",
    "            progress_med = med_progress,\n",
    "            winrate = win_rate,\n",
    "            reward_med = med_reward,\n",
    "            learn_rate = learn_rate,\n",
    "            epsilon = epsilon)\n",
    "\n",
    "        print(f'Episode: {episode}, Median progress: {med_progress}, Median reward: {med_reward}, Win rate : {win_rate}')\n",
    "\n",
    "    if not episode % SAVE_MODEL_EVERY:\n",
    "        with open(f'replay/{MODEL_NAME}.pkl', 'wb') as output:\n",
    "            pickle.dump(replay_memory, output)\n",
    "\n",
    "        model.save(f'models/{MODEL_NAME}.h5')\n",
    "    \n",
    "\n",
    "print(\"Number of Wins :\", env.n_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366a40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
