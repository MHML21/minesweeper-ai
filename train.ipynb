{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minesweeper import Board\n",
    "# from minesweeper_env import *\n",
    "from DQNsetup import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning settings\n",
    "BATCH_SIZE = 64\n",
    "learn_rate = 0.01\n",
    "LEARN_DECAY = 0.99975\n",
    "LEARN_MIN = 0.001\n",
    "DISCOUNT = 0.1 #gamma\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 0.95\n",
    "EPSILON_DECAY = 0.99975\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "# DQN settings\n",
    "CONV_UNITS = 64 # number of neurons in each conv layer\n",
    "DENSE_UNITS = 512 # number of neurons in fully connected dense layer\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "batch_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Board(4, 4)\n",
    "env.set_mines_about(2,2,1)\n",
    "state_im = env.board3D()\n",
    "model = DQN_setup(learn_rate, state_im.shape, 16, CONV_UNITS, DENSE_UNITS)\n",
    "target_model = DQN_setup(learn_rate, state_im.shape, 16, CONV_UNITS, DENSE_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dig(2,2)\n",
    "temp_state_im = state_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get action\n",
    "board = temp_state_im.reshape(1, 16)\n",
    "moves = model.predict(np.reshape(temp_state_im, (1, env.rows, env.cols, 1)))\n",
    "print(type(moves))\n",
    "print(\"moves:\", moves)\n",
    "# moves[board!=-1] = np.min(moves) # set already clicked tiles to min value\n",
    "action = np.argmax(moves)\n",
    "print(\"action:\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "new_state, gamestate = env.dig(math.floor(action / 4), action % 4)\n",
    "new_state = env.board3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 0\n",
    "if gamestate == Board.GAME_LOST or gamestate == Board.INVALID_MOVE:\n",
    "    reward = -1\n",
    "elif gamestate == Board.GAME_CONT:\n",
    "    reward = 1\n",
    "else:\n",
    "    reward = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "batch_array.append((state_im, action, reward, new_state))\n",
    "current_states = np.array([transition[0] for transition in batch_array])\n",
    "current_qs_list = model.predict(current_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_current_states = np.array([transition[3] for transition in batch_array])\n",
    "future_qs_list = target_model.predict(new_current_states)\n",
    "for i, (current_state, action, reward, new_current_states) in enumerate(batch_array):\n",
    "        if reward % 2:\n",
    "            max_future_q = np.max(future_qs_list[i])\n",
    "            new_q = reward + DISCOUNT * max_future_q\n",
    "        else:\n",
    "            new_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[i]\n",
    "        current_qs[action] = new_q\n",
    "\n",
    "        X_train.append(current_state)\n",
    "        Y_train.append(current_qs)\n",
    "\n",
    "model.fit(np.array(X_train), np.array(Y_train), batch_size=1)\n",
    "print(\"3\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
